{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Mining\n",
    "## Practical Assignment 1\n",
    "\n",
    "### BÃ¤r Halberkamp (10758380) & Steven Raaijmakers (10804242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 1: Loading the data into a Pandas Data Frame [0.5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = ['1stFlrSF','2ndFlrSF','BedroomAbvGr','TotalBsmtSF',\n",
    "            'LotFrontage','LotArea','MasVnrArea','BsmtFinSF1',\n",
    "            'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GrLivArea']\n",
    "target = ['SalePrice']\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('house_prices.csv',sep=\",\", header = 0, \n",
    "                 usecols=target + features)\n",
    "\n",
    "# features dataframes\n",
    "\n",
    "X = (df[features]).values\n",
    "\n",
    "# target dataframe\n",
    "y = np.array([df['SalePrice']])\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 2: Split the data into a training set and a test set [0.5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1022\n",
      "Test set size: 438\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training and a test set. Use a  70%-30% split.\n",
    "# Your code goes here.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n",
    "# Print the number of examples in the training set and the test set\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 3: Linear Regression [2 pts]\n",
    "\n",
    "1. Train a linear regression model\n",
    "2. Compare the importance of features based on the parameters $\\theta$ of the model\n",
    "3. Compute goodness-of-fit, $R^2$, and error, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Missing Data **: Often the data you are considering is incomplete. For example there can be the case that the people that collected the training data forgot to save the number of bedrooms for some example house in the training of test set. In this case, if you look into the datasets you will find the value *NaN*. This is not a real value, hence Linear Regression cannot handle it.\n",
    "\n",
    "The question is how can we handle missing data. There are many ways to do so, some more sophisticated than others. Here we will use a simple approach. This simple approach fills in the missing values, i.e. replaces the *NaN* by the median of the corresponding feature. E.g. if there is a *NaN* value for the number of bedrooms, this *NaN* value will be replaced by the median number of bedrooms in all other example houses in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  896.,   448.,     3., ...,   896.,   896.,  1344.],\n",
       "       [ 1575.,   626.,     4., ...,   697.,   697.,  2201.],\n",
       "       [  975.,   975.,     3., ...,   326.,   975.,  1950.],\n",
       "       ..., \n",
       "       [ 1493.,     0.,     3., ...,   484.,  1478.,  1493.],\n",
       "       [ 1117.,     0.,     3., ...,   480.,  1029.,  1117.],\n",
       "       [ 2036.,     0.,     3., ...,    80.,  2136.,  2036.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in the missing data in the dataset (i.e. replace NaN values) \n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values='NaN', strategy='median', axis=0, verbose=0, copy=False)\n",
    "imputer.fit(X_train) # replace X_train with the name of the training features array in your code\n",
    "imputer.transform(X_train) # replace X_train with the name of the training features array in your code\n",
    "imputer.transform(X_test) # replace X_test with the name of the test features array in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_0: 26222.993607.0\n",
      "Theta_1: 107.732463261\n",
      "Theta_2: 114.194416786\n",
      "Theta_3: -18139.0178311\n",
      "Theta_4: 20.1267765262\n",
      "Theta_5: 76.0628983581\n",
      "Theta_6: 0.208818526485\n",
      "Theta_7: 59.8591060606\n",
      "Theta_8: 16.9102643152\n",
      "Theta_9: -5.50597165218\n",
      "Theta_10: 8.72248386281\n",
      "Theta_11: 20.1267765263\n",
      "Theta_12: -17.0656235595\n"
     ]
    }
   ],
   "source": [
    "# Train a linear regression model\n",
    "# Your code goes here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Print the parameters theta\n",
    "# Your code goes here\n",
    "print(\"Theta_0: %f.0\" % (lr.intercept_))\n",
    "for i, t in enumerate(lr.coef_[0]):\n",
    "    print(\"Theta_\" + str(i+1) + \":\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 1**. Can you identify the 5 features that are the most important for the predicting the house price? If yes, identify and print these top-5 features. If no, explain the problem and solve it so you can answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BedroomAbvGr', '2ndFlrSF', '1stFlrSF', 'LotFrontage', 'MasVnrArea']\n"
     ]
    }
   ],
   "source": [
    "# get top 5 highest abs thetas\n",
    "top = list(reversed(sorted([abs(i) for i in lr.coef_[0]])))[:5]\n",
    "abs_f = [abs(i) for i in lr.coef_[0]]\n",
    "top_thetas = [list(abs_f).index(i) for i in top]\n",
    "\n",
    "print([features[i] for i in top_thetas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Root mean squared error\n",
    "\n",
    "Implement the root mean squared error function, without using the scikit-learn mean_squared_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 2**. What is the $R^2$ goodness-of-fit and the RMSE of your model when (a) computing them on the training set, and (b) computing them on the test set? Which one shall you trust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2\n",
      "Train = 0.665784\n",
      "Test = 0.652254\n",
      "\n",
      "RMSE\n",
      "Train = 46491.488090\n",
      "Test = 45355.460856\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(\"R^2\")\n",
    "print(\"Train = %f\" % lr.score(X_train, y_train))\n",
    "print(\"Test = %f\" % lr.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nRMSE\")\n",
    "print(\"Train = %f\" % rmse(y_train, lr.predict(X_train)))\n",
    "print(\"Test = %f\" % rmse(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is already fitted to the training set, so to check the perfomance the test set is more trustable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 4. Adding features [3 pts]\n",
    "1. Add a number of features by including polynomials and interactions of different degree\n",
    "2. Train and test the different linear regression models over the data\n",
    "3. Test whether increasing the complexity of the model overfits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Consider the original dataset that was loaded into Pandas, and then turned into a Numpy array from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement a function that constructs additional features by considering the polynomials of the original features\n",
    "# along with their interactions. degree is the degree of the polynomial.\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Implement a function that constructs additional features by considering the polynomials of the original features\n",
    "# along with their interactions. degree is the degree of the polynomial.\n",
    "\n",
    "def polynomial(X, degree):\n",
    "    X = X.reshape(-1, 1)\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False).fit(X)\n",
    "    X_poly = poly.transform(X)\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (12264,1) (1022,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3f6e6ad62e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Compute and print RMSE using your code above on the training set and on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscaled_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscaled_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-3f6e6ad62e96>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Compute and print RMSE using your code above on the training set and on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscaled_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscaled_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-368cbe277789>\u001b[0m in \u001b[0;36mrmse\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (12264,1) (1022,1) "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Generate polynomial dataset (both training and test) of degrees 1, 2, 3, 4, 5\n",
    "X_train_polys = [polynomial(X_train, p) for p in range(1, 6)]\n",
    "X_test_polys = [polynomial(X_test, p) for p in range(1, 6)]\n",
    "\n",
    "# Scale all features using the RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "scaled_train = [robust_scaler.fit_transform(x) for x in X_train_polys]\n",
    "scaled_test = [robust_scaler.fit_transform(x) for x in X_test_polys]\n",
    "\n",
    "# Compute and print RMSE using your code above on the training set and on the test set\n",
    "rmse_train = [rmse(y_train, x) for x in scaled_train]\n",
    "rmse_test = [rmse(y_test, x) for x in scaled_test]\n",
    "\n",
    "print(\"RSMEs for polynomials with degree [1, 2, 3, 4, 5]: {} (train) {} (test)\".format(rmse_train, rmse_test))\n",
    "\n",
    "# Compute and print R^2 on the training set and on the test set\n",
    "r2_train = [x.score(X_train, y_train) for x in scaled_train]\n",
    "r2_test = [x.score(X_test, y_test) for x in scaled_test]\n",
    "\n",
    "print(\"R^2s for polynomials with degree [1, 2, 3, 4, 5]: {} (train) {} (test)\".format(r2_train, r2_test))\n",
    "\n",
    "# Generate a plot with the x-axis representing the complexity of the model (i.e. the degree of the polynomial features)\n",
    "# Make the degree range from 1 to 5. The y-axis should represent the RMSE. Plot a line for degree = 1, 2, 3, 4, and 5\n",
    "# for the training error and the test error.\n",
    "plt.plot(range(1, 6), rmse_train, label=\"Training set\")\n",
    "plt.plot(range(1, 6), rmse_test, label=\"Test set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Question 3**. Write your conclusions regarding the performance of the models of increasing complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 5. Regularization [2pts]\n",
    "\n",
    "1. Feature selection using regularization\n",
    "2. Training/validation/test split\n",
    "3. Find the optimal parameter for the regularizer\n",
    "4. Compare linear regression with and without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the 2 degree polynomial features constructed above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Use regularization for feature selection; choose the \\lambda parameter (i.e. the alpha in python) equal to 500\n",
    "model = Ridge(500).fit(X_train, y_train)\n",
    "\n",
    "print(\"RMSE (training): {} (no regularization), {} (with regularization)\".format(rmse(y_train, lr.predict(X_train)), rmse(y_train, model.predict(X_train))))\n",
    "print(\"RMSE (test): {} (no regularization), {} (with regularization)\".format(rmse(y_test, lr.predict(X_test)), rmse(y_test, model.predict(X_test))))\n",
    "\n",
    "\n",
    "# Identify how many features were selected (i.e. had a non-zero parameter \\theta) by the regularized model\n",
    "# your code goes here\n",
    "print(\"Number of features used: {}\".format(np.sum(model.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 4**: Write your conclusions regarding the performance of the two models answering the questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Regularization seems to not improve the model's fit for neither the training or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compare the coefficients (i.e. the parameters \\theta) of the linear regression models with and without regularization.\n",
    "# Plot them in a graph, where the x-axis is the index of a coefficient while the y-axis the magnitute of it.\n",
    "yvals = (list(lr.coef_[0][:]), list(model.coef_[0][:]))\n",
    "yvals[0].insert(0, lr.intercept_)\n",
    "yvals[0].insert(0, model.intercept_)\n",
    "plt.plot(range(len(lr.coef_[0]) + 2) ,yvals[0], linestyle='--', marker='o', markersize=4, linewidth=2, label=\"no regularization\")\n",
    "plt.plot(range(len(model.coef_[0])) ,yvals[1], linestyle='--', marker='o', markersize=4, linewidth=2, label=\"with regularization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 5**. Write your conclusions regarding the coeeficients of the two models based on the plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Regularization seems to prioritize a different feature, which can explain it being a worse fit. It's unclear why a different feature is chosen, and a plot for different values of alpha could confirm whether that is the cause of the problems with the regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 6: Categorical features [2 pts]\n",
    "\n",
    "Consider now the entire dataset, without selecting specific features as you did in Part 1. Features in this dataset are both *numerical* and *categorical*.\n",
    "\n",
    "**Question 6**. Look at the description of the features. Which features need to be converted into numerical? Provide 5 features as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "MSZoning, Alley, Utilities, HouseStyle, Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 7**. Which features need to be converted into integers and which into one-hot encoding? Provide 3 features for each case. Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Integers:\n",
    "BsmtExposure\n",
    "BsmtQual\n",
    "BsmtCond\n",
    "\n",
    "One-hot encoding:\n",
    "Roof material\n",
    "RoofStyle\n",
    "Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Transform all categorical features into an one-hot encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('house_prices.csv',sep=\",\", header = 0)\n",
    "\n",
    "# Transform all categorical features into one-hot encoding.\n",
    "df2 = pd.get_dummies(df2)\n",
    "   \n",
    "# Split the dataset into training, validation and test sets, using 60%, 20%, and 20% splits.\n",
    "y2 = np.array([df['SalePrice']])\n",
    "y2 = y.reshape(-1, 1)\n",
    "\n",
    "X2 = np.array(df2[df2.columns.difference(['SalePrice'])])        \n",
    "\n",
    "# split data into train + validation + test \n",
    "X_trainval2, X_test2, y_trainval2, y_test2 = train_test_split(X2, y2, random_state=1, test_size=0.20)\n",
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_trainval2, y_trainval2, random_state=1, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1391.,     0.,     0., ...,  2004.,  2004.,  2008.],\n",
       "       [ 1173.,     0.,     0., ...,  1974.,  1974.,  2010.],\n",
       "       [ 1324.,     0.,     0., ...,  2006.,  2006.,  2009.],\n",
       "       ..., \n",
       "       [  902.,   808.,     0., ...,  1921.,  1950.,  2010.],\n",
       "       [  698.,     0.,     0., ...,  1947.,  2008.,  2009.],\n",
       "       [ 1022.,  1038.,   168., ...,  1999.,  1999.,  2009.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer2 = Imputer(missing_values='NaN', strategy='median', axis=0, verbose=0, copy=False)\n",
    "imputer2.fit(X_train2)\n",
    "imputer2.transform(X_train2)\n",
    "imputer2.transform(X_test2)\n",
    "imputer2.transform(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Normalize all features based on the training and validation set combined.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1)).fit(X_train2)\n",
    "X_train_scaled = min_max_scaler.transform(X_train2)\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1)).fit(X_valid2)\n",
    "X_valid_scaled = min_max_scaler.transform(X_valid2)\n",
    "\n",
    "# Train a linear regression model on the training and validation set combined. \n",
    "X_train_valid = np.concatenate((X_train_scaled, X_valid_scaled), axis=0)\n",
    "y_train_valid = np.concatenate((y_train2, y_valid2), axis=0)\n",
    "lr2 = LinearRegression().fit(X_train_valid, y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tR^2: 0.888786741421 \tRMSE: 24452.996024\n",
      "2 \tR^2: 0.897614369455 \tRMSE: 23462.4466939\n",
      "3 \tR^2: 0.901060522705 \tRMSE: 23064.2108739\n",
      "4 \tR^2: 0.902986316924 \tRMSE: 22838.6427213\n",
      "5 \tR^2: 0.904232389428 \tRMSE: 22691.4955451\n",
      "6 \tR^2: 0.905097361045 \tRMSE: 22588.7884839\n",
      "7 \tR^2: 0.905718926274 \tRMSE: 22514.6942824\n",
      "8 \tR^2: 0.906171773522 \tRMSE: 22460.5583389\n",
      "9 \tR^2: 0.906501095562 \tRMSE: 22421.1072114\n",
      "10 \tR^2: 0.906736436001 \tRMSE: 22392.8720262\n",
      "11 \tR^2: 0.906898235555 \tRMSE: 22373.439306\n",
      "12 \tR^2: 0.907001256646 \tRMSE: 22361.0572952\n",
      "13 \tR^2: 0.907056521539 \tRMSE: 22354.4122318\n",
      "14 \tR^2: 0.907072480069 \tRMSE: 22352.4930069\n",
      "15 \tR^2: 0.90705574982 \tRMSE: 22354.5050371\n",
      "16 \tR^2: 0.907011605296 \tRMSE: 22359.8131215\n",
      "17 \tR^2: 0.906944312663 \tRMSE: 22367.9021877\n",
      "18 \tR^2: 0.906857365632 \tRMSE: 22378.3495267\n",
      "19 \tR^2: 0.906753655886 \tRMSE: 22390.8046568\n",
      "\n",
      "Best alpha: 14\n"
     ]
    }
   ],
   "source": [
    "# Train a regularized model (use Ridge regularization). To decide the parameter alpha, use a range of values to train,\n",
    "# the model. Test its performance on the validation set. Output the RMSE for different values of alpha, and choose the\n",
    "# best value.\n",
    "# your code goes here\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = list(np.arange(1, 20, 1))\n",
    "r2s = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha).fit(X_train2, y_train2)\n",
    "    r2 = ridge.score(X_valid2, y_valid2)\n",
    "    rmse = np.sqrt(((y_valid2 - ridge.predict(X_valid2)) ** 2).mean())\n",
    "    r2s.append(r2)\n",
    "    print(alpha, \"\\tR^2:\", r2, \"\\tRMSE:\", rmse)\n",
    "\n",
    "\n",
    "best_alpha = alphas[r2s.index(max(r2s))]\n",
    "print(\"\\nBest alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train the regularized model (use Ridge regularization) with the best alpha parameter found above on the traing and \n",
    "# validation set combined.\n",
    "# your code goes here\n",
    "ridge = Ridge(best_alpha).fit(X_train_valid, y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lr2: -396744797980444393472.000000\n",
      "Reg: -10931278.086818\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of the linear regression with and without regularization\n",
    "# your code goes here\n",
    "lr2_score = lr2.score(X_test2, y_test2)\n",
    "reg_score = ridge.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Lr2: %f\" % lr2_score)\n",
    "print(\"Reg: %f\" % reg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 6**: What are your conclusions for the two model compared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The regularized model has a better score (R^2) because it is more close to zero, thus the prediction using the regularized model will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
